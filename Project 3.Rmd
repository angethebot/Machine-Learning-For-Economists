---
title: "Prediction Competition 4"
author: "Tomato 787"
date: "2024-02-14"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
  word_document: default
---

## 1. Name: Tomato787\
## 2. MSE=0.05554; R2=0.8263  \
## 3. Graph for Q2 as calculated from the training data \
Variable importance is measured by the increase in R-sqaured  when only the single variable is put in.


\newpage
## 4. Graph for Q3 as calculated from the training data.\
For the dsitribution graph, I generate 14 panels for each variable (1 response variable and 13 features) from training data as well as the test data.

\newpage
To visualize the link between all variables as well as the link between each feature and the response variable (price), I first used a heatmap and then scattered the correlation scatterplot of any two variables.


\newpage
## 5.Screenshot of an example from ChatGPT/GPT4 interaction.\

\newpage
\newpage

## 6. Code for Q1, Q2 and Q3 answers.\
The MSE and R-squared for four methods:\

|                     |     MSE     |      R-sqaured      |
|---------------------|-------------|---------------------|
| Linear regression   |  0.0555368  |      0.8262791      |
|---------------------|-------------|---------------------|
| LASSO               |  0.05712359 |      0.8262439      |
|---------------------|-------------|---------------------|
| Ridge               |  0.05839724 |      0.8236963      |
|---------------------|-------------|---------------------|
| Subset Selection    |  0.0571107  |      0.8262772      |
|---------------------|-------------|---------------------|

Linear regression has the lowest MSE and highest R-sqaured, I choose Linear Regression as my final model.


```{r setup, include=FALSE, echo=TRUE}
library(formatR)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
getwd()

######################
## Set your wd here ##
######################
path_global="D:/Angie/Grad in UWaterloo/Term 2 W24/ECON626/Prediction Competition/PC4"
setwd(path_global)

```
### Cleaning the data
```{r, tidy=TRUE}
suppressPackageStartupMessages(library(tidyverse))
# Load data
car_train_l=read.csv("pc3_and_pc4_training_data_large_v1.csv")
car_train_s=read.csv("pc3_and_pc4_training_data_small_v1.csv")
car_test=read.csv("pc4_test_data_without_response_v1.csv")

#Check the variable types & Missing values
library(skimr)
#skim_without_charts(car_train_l)
#skim_without_charts(car_train_s)
#skim_without_charts(car_test)
#Both training datasets have 18 variables: 10 character variables and 8 numeric variables


#In the 10 categorical variables, actually only 5: body_type, exterior_color, fuel_type, listed_date, and wheel_system are categorical; the rest are just numeric with unit number:I need to convert them into numeric first.
false_cat_var = c("back_legroom", "height","length","wheelbase","width")
extract_numeric = function(x) {
  # Remove non-numeric characters using regex
  numeric_part = as.numeric(gsub("[^0-9.]", "", x))
  return(numeric_part)
}
# Apply the function to selected columns of the dataframe
car_train_l[false_cat_var] = lapply(car_train_l[false_cat_var], extract_numeric)
car_train_s[false_cat_var] = lapply(car_train_s[false_cat_var], extract_numeric)
car_test[false_cat_var] = lapply(car_train_s[false_cat_var], extract_numeric)

#Check the variable types & Missing values
# skim_without_charts(car_train_l)
# skim_without_charts(car_train_s)
# skim_without_charts(car_test)

# There are still missing values in back_legroom, height, length, wheelbase, and width. I will simply replace the value with the average of that column.
car_train_l = car_train_l %>% mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE)))
car_train_s = car_train_s %>% mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE)))
car_test = car_test %>% mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE)))

# For categorical variable: listed_date: I can calculate the age of the car with listed_date and year. and I'll keep the age variable, and not use listed_date and year.
car_train_l$sale_year = as.numeric(substr(car_train_l$listed_date, start = 1, stop = 4))
car_train_l$age = car_train_l$sale_year - car_train_l$year
car_train_s$sale_year = as.numeric(substr(car_train_s$listed_date, start = 1, stop = 4))
car_train_s$age = car_train_s$sale_year - car_train_s$year
car_test$sale_year = as.numeric(substr(car_test$listed_date, start = 1, stop = 4))
car_test$age = car_test$sale_year - car_test$year

# I won't use the variable exterior_color as it contains too many categories since different companies name the same color differently; and color matters less for the price of used cars.

# For the rest 3 categorical variable: body_type, fuel_type, and wheel_system, I encode them with numbers so that they can be numeric variables and I can later on scale them.
# table(car_train_l$body_type)
# table(car_train_s$body_type)
# table(car_test$body_type)
#same types for each dataset. if we just as.factor() them separately, it should be fine.
car_train_l$body_type_num = as.numeric(as.factor(car_train_l$body_type))
car_train_s$body_type_num = as.numeric(as.factor(car_train_s$body_type))
car_test$body_type_num = as.numeric(as.factor(car_test$body_type))

car_train_l$fuel_type_num = as.numeric(as.factor(car_train_l$fuel_type))
car_train_s$fuel_type_num = as.numeric(as.factor(car_train_s$fuel_type))
car_test$fuel_type_num = as.numeric(as.factor(car_test$fuel_type))

car_train_l$wheel_system_num = as.numeric(as.factor(car_train_l$wheel_system))
car_train_s$wheel_system_num = as.numeric(as.factor(car_train_s$wheel_system))
car_test$wheel_system_num = as.numeric(as.factor(car_test$wheel_system))


# head(car_train_l)
# Keep the features I'll use. (now all features are numeric now, I just need to select)
# price, back_legroom, engine_displacement, height, highway_fuel_economy, horsepower,length,mileage,wheelbase,width,age, body_type_num,fuel_type_num,wheel_system_num
library(tidyverse)
car_train_l2=car_train_l%>%dplyr::select(price, back_legroom, engine_displacement, height, highway_fuel_economy, horsepower, length, mileage, wheelbase, width, age, body_type_num, fuel_type_num, wheel_system_num)
car_train_s2=car_train_s%>%dplyr::select(price, back_legroom, engine_displacement, height, highway_fuel_economy, horsepower, length, mileage, wheelbase, width, age, body_type_num, fuel_type_num, wheel_system_num)
car_test2=car_test%>%dplyr::select(price, back_legroom, engine_displacement, height, highway_fuel_economy, horsepower, length, mileage, wheelbase, width, age, body_type_num, fuel_type_num, wheel_system_num)

# Scale all the features (No need in this PC)

# Specify the names of the numeric columns you want to scale
#var_scale = c("back_legroom", "engine_displacement", "height", "highway_fuel_economy", "horsepower", "length", "mileage", "wheelbase", "width", "age", "body_type_num", "fuel_type_num", "wheel_system_num")

# Method 1: centering it around its mean and then dividing by its standard deviation. 
# Scale the specified numeric columns
#car_train_l3 = car_train_l2
#car_train_l3[var_scale] = scale(car_train_l3[var_scale])
#car_train_s3 = car_train_s2
#car_train_s3[var_scale] = scale(car_train_s3[var_scale])
#car_test3 = car_test2
#car_test3[var_scale] = scale(car_test2[var_scale])

# Method 2: Min-max scaling scales the values to a specific range (usually 0 to 1) without altering their sign.
#car_train_l4 = car_train_l2
#car_train_l4[var_scale] = apply(car_train_l4[var_scale], 2, function(x) (x - min(x)) / (max(x) - min(x)))
#car_train_s4 = car_train_l2
#car_train_s4[var_scale] = apply(car_train_s4[var_scale], 2, function(x) (x - min(x)) / (max(x) - min(x)))
#car_test4 = car_test2
#car_test4[var_scale] = apply(car_test4[var_scale], 2, function(x) (x - min(x)) / (max(x) - min(x)))
```


### Train models and predict

```{r, tidy=TRUE}
# Since it takes too long to use both large dataset and small dataset, I'll just use small dataset and split it into training and test datasets.
set.seed(6264)  # For reproducibility

# Use the unscaled data
train_index_unsc = sample(nrow(car_train_s2), 0.5 * nrow(car_train_s2))  # 50% for training, 50% for testing
train_data_unsc = car_train_s2[train_index_unsc, ]
test_data_unsc = car_train_s2[-train_index_unsc, ]
nrow(test_data_unsc)
nrow(train_data_unsc)

# Log transform the unscaled data
train_data_unsc$price = log(train_data_unsc$price)
test_data_unsc$price = log(test_data_unsc$price)
car_test2$price = log(car_test$price)



head(train_data_unsc)
head(test_data_unsc)
```


### Data Exploration
```{r, tidy=TRUE}
# I want to see the distribution of all the features and my response variable in training data and test data.
library(ggplot2)

# Combine train and test datasets, adding a column to identify dataset
train = train_data_unsc
test = car_test2
train$dataset = "Train"
test$dataset = "Test"
combined <- rbind(train, test)

# Create a list to store plots
plots <- list()

# Loop through each variable
for (var in names(train)[names(train) != "dataset"]) {
  # Create a plot for the variable
  p <- ggplot(combined, aes_string(x = var, fill = "dataset")) +
    geom_density(alpha = 0.5) +
    labs(title = paste("Distribution of", var), x = var, y = "Density") +
    theme_minimal() +
    facet_grid(. ~ dataset)
  # Add the plot to the list
  plots[[var]] <- p
}

# Print the plots
# print(plots)

```

```{r, tidy=TRUE}
# Correlation

# I want to see the correlation between all the variables (features and response variable) in training data.
corr_train = round(cor(train_data_unsc),2)

library(reshape2)
melted_corr_train<- melt(corr_train)
head(melted_corr_train)

library(ggplot2)
ggplot(data = melted_corr_train, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

# a correlation matrix has redundant information. We’ll use the functions below to set half of it to NA.
# Get lower triangle of the correlation matrix
  get_lower_tri<-function(corr_train){
    corr_train[upper.tri(corr_train)] <- NA
    return(corr_traint)
  }
  # Get upper triangle of the correlation matrix
  get_upper_tri <- function(corr_train){
    corr_train[lower.tri(corr_train)]<- NA
    return(corr_train)
  }
  upper_tri <- get_upper_tri(corr_train)
# Melt the correlation matrix
library(reshape2)
melted_corr_train2 <- melt(upper_tri, na.rm = TRUE)
# Heatmap
library(ggplot2)
ggheatmap <-ggplot(data = melted_corr_train2, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1))+
 coord_fixed()


ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))


# Now I want to see the correlation between the features and the response variable
response_var <- "price"
feature_vars <- c("back_legroom", "engine_displacement", "height", "highway_fuel_economy", 
                  "horsepower", "length", "mileage", "wheelbase", "width", "age", 
                  "body_type_num", "fuel_type_num", "wheel_system_num")

# For Train data train_data_unsc
library("ggpubr")

s1 = ggscatter(train_data_unsc, x = "back_legroom", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "back_legroom")

s2 = ggscatter(train_data_unsc, x = "engine_displacement", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "engine_displacement")

s3 = ggscatter(train_data_unsc, x = "height", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "height")

s4 = ggscatter(train_data_unsc, x = "highway_fuel_economy", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "highway_fuel_economy")

s5 = ggscatter(train_data_unsc, x =  "horsepower", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab =  "horsepower")

s6 = ggscatter(train_data_unsc, x = "length", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "length")

s7 = ggscatter(train_data_unsc, x = "mileage", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "mileage")

s8 = ggscatter(train_data_unsc, x = "wheelbase", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "wheelbase")

s9 = ggscatter(train_data_unsc, x = "width", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "width")

s10 = ggscatter(train_data_unsc, x = "age", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "age")

s11 = ggscatter(train_data_unsc, x = "body_type_num", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "body_type_num")

s12 = ggscatter(train_data_unsc, x = "fuel_type_num", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "fuel_type_num")

s13 = ggscatter(train_data_unsc, x = "wheel_system_num", y = "price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "wheel_system_num")

#s1
#s2
#s3
#s4
#s5
#s6
#s7
#s8
#s9
#s10
#s11
#s12
#s13

```
### Variable Importance
```{r, tidy=TRUE}
# I'll just use the unscaled data for the total dataset (small)

# Variable importance: use the R square when only the single variable is put in.
# Load required library
library(car)

# Step 1: Initialize an empty vector to store increase in R-squared
rsq_increase = c()
df_lm = car_train_s2
# colnames(df_lm)
# ncol(df_lm)

# Step 2: Iterate through each feature and calculate increase in R-squared
for (i in 2:14) {
  feature = names(df_lm)[i]
  model = lm(price ~ df_lm[[feature]], data = df_lm)
  rsq = summary(model)$r.squared
  rsq_increase=c(rsq_increase,rsq)
}

var_imp_lm = data.frame(names(df_lm)[2:14],rsq_increase)
colnames(var_imp_lm) = c("variable", "importance")
var_imp_lm = arrange(var_imp_lm,desc(importance))
var_imp_lm   
# We can know that from the most important to least important variables, we have: horsepower, mileage, age, width, engine_displacement, wheelbase, length,highway_fuel_economy, wheel_system_num, height, back_legroom, body_type_num, fuel_type_num
# Reordering groups in a ggplot2 chart can be a struggle. This is due to the fact that ggplot2 takes into account the order of the factor levels, not the order you observe in your data frame. You can sort your input data frame with sort() or arrange(), it will never have any impact on your ggplot2 output.

var_imp_lm$variable <- factor(var_imp_lm$variable, levels = var_imp_lm$variable[order(var_imp_lm$importance, decreasing = FALSE)])
jpg_var_imp_lm = ggplot2::ggplot(var_imp_lm) +
  geom_col(aes(x = variable, y = importance),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()+
  ggtitle("Linear Regression Variable Importance Graph")

```
### Linear Regression
MSE: 0.0555368 
RMSE: 0.2356625 
R-squared: 0.8262791
```{r, tidy=TRUE}
# Load required libraries
library(boot)

# Assuming your data frame is named df

# Step 1: Split data into training and testing sets
train_data = train_data_unsc
test_data = test_data_unsc
head(train_data)
# Step 2: Fit linear regression model using the training data
lm_model <- lm(price ~ ., data = train_data)

# Step 3: Evaluate model performance on the testing data
predicted <- predict(lm_model, newdata = test_data)
# Calculate evaluation metrics
mse <- mean((test_data$price - predicted)^2)  # Mean Squared Error
rmse <- sqrt(mse)  # Root Mean Squared Error
r_squared <- summary(lm_model)$r.squared  # R-squared

# Print evaluation metrics
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")

# Step 4: Perform cross-validation on different kinds of linear regression 
lm_model2 <- lm(price ~ horsepower+mileage+age+width+engine_displacement+wheelbase+length+highway_fuel_economy, data = train_data)
predicted2 <- predict(lm_model2, newdata = test_data)
# Calculate evaluation metrics
mse2 <- mean((test_data$price - predicted2)^2)  # Mean Squared Error
rmse2 <- sqrt(mse2)  # Root Mean Squared Error
r_squared2 <- summary(lm_model2)$r.squared  # R-squared

cat("MSE:", mse2, "\n")
cat("RMSE:", rmse2, "\n")
cat("R-squared:", r_squared2, "\n")


# The MSE and R-sqaure for test data does not get better if I reduce any more variables. So my final model will be the linear regression with all the data
predicted_lm <- predict(lm_model, newdata = car_test2)
write.csv(predicted_lm,file = "predicted_lm_csv_tomato787.csv", row.names=FALSE, col.names = FALSE) 

```

### LASSO
MSE: 0.05712359 
R-squared: 0.8262439 
```{r, tidy=TRUE}
library(glmnet)
train_data_lasso = train_data_unsc
test_data_lasso = train_data_unsc

# Step 1: Load the Data
x_train <- as.matrix(train_data_lasso[, -1]) # Exclude response variable
y_train <- train_data_lasso$price
x_test <- as.matrix(test_data_lasso[, -1])   # Exclude response variable
y_test <- test_data_lasso$price

# Step 2: Fit the Lasso regression model
# Tutorial: https://www.statology.org/lasso-regression-in-r/
# Next, we’ll use the glmnet() function to fit the lasso regression model and specify alpha=1. Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 
# To determine what value to use for lambda, we’ll perform k-fold cross-validation and identify the lambda value that produces the lowest test mean squared error (MSE). Note that the function cv.glmnet() automatically performs k-fold cross validation using k = 10 folds.

#perform k-fold cross-validation to find optimal lambda value
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1) 

#find optimal lambda value that minimizes test MSE
best_lambda <- lasso_model$lambda.min
best_lambda
#produce plot of test MSE by lambda value
plot(lasso_model) 


# Step 3: Analyze the final model
#find coefficients of best model
best_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
coef(best_model)

# Step 4: calculate test MSE and R-sqaured
# Predict on test data
predictions_lasso <- predict(best_model, newx = x_test)
# Evaluate model performance
# For example, calculate RMSE
mse_lasso <- mean((y_test - predictions_lasso)^2)
cat("MSE:", mse_lasso, "\n")
# Alternatively, you can calculate R-squared
rsquared_lasso <- cor(y_test, predictions_lasso)^2
cat("R-squared:", rsquared_lasso, "\n")



```
### Ridge
MSE: 0.05839724 
R-squared: 0.8236963
```{r, tidy=TRUE}
library(glmnet)
train_data_ridge = train_data_unsc
test_data_ridge = train_data_unsc

# Tutorial: https://www.statology.org/ridge-regression-in-r/

# Step 1: Load the data
# Convert data to matrix format
x_train_r <- as.matrix(train_data_ridge[, -1]) # Exclude response variable
y_train_r <- train_data_ridge$price
x_test_r <- as.matrix(test_data_ridge[, -1])   # Exclude response variable
y_test_r <- test_data_ridge$price

# Step 2: Fit the Ridge Regression Model with cross-validation
# Next, we’ll use the glmnet() function to fit the ridge regression model and specify alpha=0.Note that setting alpha equal to 1 is equivalent to using Lasso Regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. Also note that ridge regression requires the data to be standardized such that each predictor variable has a mean of 0 and a standard deviation of 1.Fortunately glmnet() automatically performs this standardization for you. If you happened to already standardize the variables, you can specify standardize=False.
# we’ll identify the lambda value that produces the lowest test mean squared error (MSE) by using k-fold cross-validation. Fortunately, glmnet has the function cv.glmnet() that automatically performs k-fold cross validation using k = 10 folds.
ridge_model <- cv.glmnet(x_train_r, y_train_r, alpha = 0)  # alpha = 0 for Ridge
# Plot cross-validation results
plot(ridge_model)
# Select lambda (penalty parameter) based on cross-validation
best_lambda <- ridge_model$lambda.min  # or choose lambda.1se for more regularized model
best_lambda


# Step 3: Use the final model to predict
# Fit Ridge model with selected lambda
ridge_fit <- glmnet(x_train_r, y_train_r, alpha = 0, lambda = best_lambda)
# Predict on test data
predictions_r <- predict(ridge_fit, newx = x_test)


# Step 4: Evaluate model performance
rmse_r <- mean((y_test_r - predictions_r)^2)
cat("MSE:", rmse_r, "\n")

rsquared_r <- cor(y_test_r, predictions_r)^2
cat("R-squared:", rsquared_r, "\n")

```

### Subset Selection
RMSE: 0.0571107 
R-squared: 0.8262772
```{r, tidy=TRUE}
library(MASS)
train_data_ss = train_data_unsc
test_data_ss = train_data_unsc


# Perform stepwise regression on the training data
step_model <- stepAIC(lm(price ~ ., data = train_data_ss))

# Print the summary of the model
summary(step_model)

# Make predictions on the test data
predictions_ss <- predict(step_model, newdata = test_data_ss)

# Evaluate model performance
# For example, calculate RMSE
rmse_ss <- mean((test_data_ss$price - predictions_ss)^2)
cat("RMSE:", rmse_ss, "\n")
rsquared_ss <- cor(test_data_ss$price, predictions_ss)^2
cat("R-squared:", rsquared_ss, "\n")

# Use this model to predict 
# car_test$price = log(car_test$price)
predicted_ss <- predict(step_model, newdata = car_test)
write.csv(predicted_ss,file = "predicted_ss_csv_tomato787.csv", row.names=FALSE, col.names = FALSE) 


```

